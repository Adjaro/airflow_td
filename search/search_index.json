{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 \u00c9tape 1 : Installation d'Apache Airflow sous Windows avec Docker","text":""},{"location":"#prerequis","title":"\ud83d\udccc Pr\u00e9requis","text":"<p>Avant d'installer Apache Airflow, assurez-vous d'avoir :</p> <ul> <li>Docker Desktop install\u00e9 et en cours d'ex\u00e9cution (T\u00e9l\u00e9charger ici)</li> </ul>"},{"location":"#-docs","title":"- Docs","text":""},{"location":"#etape-1-creer-un-dossier-pour-airflow","title":"\ud83d\ude80 \u00c9tape 1 : Cr\u00e9er un dossier pour Airflow","text":"<p>Dans un terminal, ex\u00e9cutez :</p> <pre><code>mkdir airflow-docker\ncd airflow-docker\n</code></pre>"},{"location":"#etape-2-telecharger-le-fichier-docker-composeyaml","title":"\ud83d\udce5 \u00c9tape 2 : T\u00e9l\u00e9charger le fichier <code>docker-compose.yaml</code>","text":"<p>Dans PowerShell ou Git Bash :</p> <pre><code>curl -LO https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml\n</code></pre> <p>Si <code>curl</code> ne fonctionne pas, t\u00e9l\u00e9chargez le fichier manuellement depuis : \ud83d\udd17 Lien de t\u00e9l\u00e9chargement</p> <p>Modifier la ligne 71 du <code>docker-compose.yml</code> pour ajouter les d\u00e9pendances : :</p> <p> <pre><code>    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- pandas requests scikit-learn numpy logging}\n</code></pre> </p>"},{"location":"#etape-3-creer-les-dossiers-necessaires","title":"\ud83d\udcc2 \u00c9tape 3 : Cr\u00e9er les dossiers n\u00e9cessaires","text":"<p>Ex\u00e9cutez la commande suivante pour cr\u00e9er les r\u00e9pertoires (ou dossier) : <pre><code>mkdir dags, logs, plugins, data\n</code></pre></p> <p>Si PowerShell ne supporte pas plusieurs dossiers, ex\u00e9cutez-les s\u00e9par\u00e9ment : <pre><code>mkdir dags\nmkdir logs\nmkdir plugins\nmkdir data\n</code></pre></p>"},{"location":"#etape-4-configurer-les-variables-denvironnement","title":"\ud83d\udee0\ufe0f \u00c9tape 4 : Configurer les variables d'environnement","text":"<p>Sous terminal vsCode : <pre><code>Set-Content .env \"AIRFLOW_UID=50000`nAIRFLOW_GID=0\"\n</code></pre></p> <p>V\u00e9rifiez que le fichier <code>.env</code> est bien cr\u00e9\u00e9 avec : <pre><code>Get-Content .env\n</code></pre> Vous devriez voir : <pre><code>AIRFLOW_UID=50000\nAIRFLOW_GID=0\n</code></pre></p>"},{"location":"#etape-5-initialiser-airflow","title":"\ud83c\udfd7\ufe0f \u00c9tape 5 : Initialiser Airflow","text":"<p>Ex\u00e9cutez la commande suivante pour initialiser la base de donn\u00e9es d'Airflow : <pre><code>docker-compose up airflow-init\n</code></pre></p>"},{"location":"#etape-6-lancer-airflow","title":"\u25b6\ufe0f \u00c9tape 6 : Lancer Airflow","text":"<p>Une fois l'initialisation termin\u00e9e, d\u00e9marrez Airflow avec : <pre><code>docker-compose up\n</code></pre></p> <p>Airflow sera accessible \u00e0 l'adresse : \ud83d\udd17 http://localhost:8080</p> <p>Par d\u00e9faut, les identifiants sont :</p> <ul> <li>Utilisateur : <code>airflow</code></li> <li>Mot de passe : <code>airflow</code></li> </ul>"},{"location":"#etape-7-arreter-airflow","title":"\ud83d\uded1 \u00c9tape 7 : Arr\u00eater Airflow","text":"<p>Si vous souhaitez arr\u00eater Airflow, utilisez : <pre><code>docker-compose down\n</code></pre></p> <p>Pour tout red\u00e9marrer plus tard : <pre><code>docker-compose up\n</code></pre></p>"},{"location":"#etape-8-nettoyage-et-suppression-des-containers","title":"\ud83d\udd04 \u00c9tape 8 : Nettoyage et suppression des containers","text":"<p>Si vous souhaitez r\u00e9initialiser compl\u00e8tement Airflow, ex\u00e9cutez : <pre><code>docker-compose down --volumes --remove-orphans\n</code></pre> Cela supprimera les volumes li\u00e9s \u00e0 Airflow.</p>"},{"location":"#resume-des-commandes-importantes","title":"\ud83c\udfaf R\u00e9sum\u00e9 des commandes importantes","text":"Commande Description <code>mkdir dags logs plugins</code> Cr\u00e9e les dossiers n\u00e9cessaires <code>curl -LO ...</code> T\u00e9l\u00e9charge le fichier docker-compose.yaml <code>Set-Content .env ...</code> Configure les variables d'environnement <code>docker-compose up airflow-init</code> Initialise Airflow <code>docker-compose up</code> D\u00e9marre Airflow <code>docker-compose down</code> Arr\u00eate Airflow <code>docker-compose down --volumes --remove-orphans</code> Supprime tous les volumes Airflow <p> \u2728 Airflow est maintenant install\u00e9 et op\u00e9rationnel sous Windows ! \ud83d\ude80 </p>"},{"location":"Cr%C3%A9ation_DAG_Simple/","title":"\ud83d\ude80 \u00c9tape 2 : Cr\u00e9ation d'un DAG Simple","text":""},{"location":"Cr%C3%A9ation_DAG_Simple/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cr\u00e9er un DAG qui permet d'explorer l'environnement d'Airflow et de comprendre les concepts de base.</p>"},{"location":"Cr%C3%A9ation_DAG_Simple/#ressources","title":"\ud83d\udcda Ressources","text":"<ul> <li>Documentation officielle d'Airflow</li> </ul>"},{"location":"Cr%C3%A9ation_DAG_Simple/#exercices-pratiques","title":"\ud83d\udcdd Exercices Pratiques","text":""},{"location":"Cr%C3%A9ation_DAG_Simple/#exercice-1-configuration-initiale","title":"Exercice 1 : Configuration initiale","text":""},{"location":"Cr%C3%A9ation_DAG_Simple/#description","title":"Description","text":"<p>Nous allons cr\u00e9er notre premier DAG Airflow avec une configuration de base.</p> <pre><code>from datetime import datetime, timedelta\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    # \u00c0 compl\u00e9ter\n}\n\ndag = DAG(\n    # \u00c0 compl\u00e9ter\n)\n\ntache_afficher_message = BashOperator(\n    # \u00c0 compl\u00e9ter\n)\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_Simple/#taches-a-realiser","title":"T\u00e2ches \u00e0 r\u00e9aliser","text":"<ol> <li>Cr\u00e9er un nouveau fichier <code>simple_dag.py</code> dans le dossier <code>dags</code>.</li> <li> <p>D\u00e9finir les arguments suivants dans <code>default_args</code>:</p> <ul> <li> <p><code>owner</code>: votre nom</p> </li> <li> <p><code>retries</code>: 2</p> </li> <li> <p><code>retry_delay</code>: 10 minutes</p> </li> <li> <p><code>start_date</code>: date actuelle</p> </li> </ul> </li> <li> <p>Cr\u00e9er une instance de DAG avec:</p> <ul> <li> <p>ID: <code>'simple_dag'</code></p> </li> <li> <p>Arguments par d\u00e9faut configur\u00e9s <code>default_args=default_args</code></p> </li> <li> <p>Description personnalis\u00e9e <code>description='.....'</code></p> </li> <li> <p>Intervalle d'ex\u00e9cution: 1 jour <code>schedule_interval=timedelta(days=1)</code></p> </li> </ul> </li> <li> <p>Ajouter une t\u00e2che <code>BashOperator</code> qui affiche \"Bonjour, Airflow!\" nommer  <code>afficher_message</code>.</p> <ul> <li>BashOperator Docs</li> </ul> </li> </ol> Astuce <ul> <li>Le fichier doit \u00eatre plac\u00e9 dans le dossier <code>dags</code>.</li> <li>Utilisez <code>catchup=False</code> pour \u00e9viter l'ex\u00e9cution des DAGs historiques.</li> </ul>"},{"location":"Cr%C3%A9ation_DAG_Simple/#solution","title":"Solution","text":"Afficher la solution <pre><code>from datetime import datetime, timedelta\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'votre_nom',\n    'retries': 2,\n    'retry_delay': timedelta(minutes=10),\n    'start_date': datetime.now(),\n}\n\ndag = DAG(\n    'simple_dag',\n    default_args=default_args,\n    description='Mon premier DAG Airflow',\n    schedule_interval=timedelta(days=1),\n    catchup=False\n)\n\ntache_afficher_message = BashOperator(\n    task_id='afficher_message',\n    bash_command='echo \"Bonjour, Airflow!\"',\n    dag=dag\n)\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_Simple/#exercice-2-integration-python","title":"Exercice 2 : Int\u00e9gration Python","text":""},{"location":"Cr%C3%A9ation_DAG_Simple/#description_1","title":"Description","text":"<p>Ajoutez des t\u00e2ches Python \u00e0 votre DAG pour \u00e9tendre ses fonctionnalit\u00e9s.</p> <pre><code>from airflow.operators.python import PythonOperator\nimport logging\n\ndef generer_message():\n   logging.info(\"Ex\u00e9cution de la t\u00e2che Python\")\n   return \"T\u00e2che ex\u00e9cut\u00e9e avec succ\u00e8s\"\n\ntache_python = PythonOperator(\n   # \u00c0 compl\u00e9ter\n)\n\n# D\u00e9finition des d\u00e9pendances\n# \u00c0 compl\u00e9ter\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_Simple/#taches-a-realiser_1","title":"T\u00e2ches \u00e0 r\u00e9aliser","text":"<ol> <li>Ajouter le code ci-dessus dans <code>simple_dag.py</code></li> <li>Cr\u00e9er la fonction Python <code>generer_message()</code> qui utilise le logger pour afficher un message</li> <li>Ajouter une nouvelle t\u00e2che <code>tache_python</code> utilisant <code>PythonOperator</code> avec la fonction <code>generer_message()</code></li> <li>D\u00e9finir les d\u00e9pendances pour que <code>tache_python</code> s'ex\u00e9cute apr\u00e8s <code>tache_afficher_message</code></li> </ol> Astuce <p>Pour d\u00e9finir l'ordre d'ex\u00e9cution des t\u00e2ches, utilisez les op\u00e9rateurs de d\u00e9pendance:</p> <ul> <li>Syntaxe: <code>tache_1 &gt;&gt; tache_2</code> (tache_2 s'ex\u00e9cute apr\u00e8s tache_1) ou</li> <li>Syntaxe: <code>tache_2 &gt;&gt; tache_1</code> </li> </ul>"},{"location":"Cr%C3%A9ation_DAG_Simple/#solution_1","title":"Solution","text":"Afficher la solution <pre><code>import logging\nfrom datetime import datetime, timedelta\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\n\ndefault_args = {\n    'owner': 'votre_nom',\n    'retries': 2,\n    'retry_delay': timedelta(minutes=10),\n    'start_date': datetime.now(),\n}\n\ndag = DAG(\n    'simple_dag',\n    default_args=default_args,\n    description='Mon premier DAG Airflow',\n    schedule_interval=timedelta(days=1),\n    catchup=False\n)\n\ntache_afficher_message = BashOperator(\n    task_id='afficher_message',\n    bash_command='echo \"Bonjour, Airflow!\"',\n    dag=dag\n)\n\n\ndef generer_message():\n    logging.info(\"Ex\u00e9cution de la t\u00e2che Python\")\n    return \"T\u00e2che ex\u00e9cut\u00e9e avec succ\u00e8s\"\n\ntache_python = PythonOperator(\n    task_id='tache_python',\n    python_callable=generer_message,\n    dag=dag\n)\n\n# D\u00e9finition des d\u00e9pendances\ntache_afficher_message &gt;&gt; tache_python\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_Simple/#verification","title":"\ud83d\udd0d V\u00e9rification","text":"<p>Pour valider votre DAG :</p> <ol> <li> <p>V\u00e9rifiez sa pr\u00e9sence dans l'interface web d'Airflow.</p> <ul> <li>http://localhost:8080/</li> </ul> </li> <li> <p>Connectez-vous avec vos identifiants :</p> <ul> <li>Utilisateur : <code>airflow</code></li> <li>Mot de passe : <code>airflow</code></li> </ul> </li> <li> <p>Recherchez votre DAG :</p> <ul> <li>Dans la liste des DAGs disponibles, trouvez <code>simple_dag</code></li> <li></li> </ul> </li> <li> <p>D\u00e9clenchement et surveillance :</p> <ul> <li>D\u00e9clenchez manuellement le DAG via l'interface</li> </ul> </li> <li> <p>Visualisez le graphe :</p> <ul> <li>Acc\u00e9dez \u00e0 l'onglet \"Graph View\"</li> <li>Vous devriez voir la repr\u00e9sentation visuelle de votre DAG</li> <li></li> </ul> </li> <li> <p>Analysez les logs :</p> <ul> <li>Double-cliquez sur chaque t\u00e2che pour acc\u00e9der aux logs</li> <li>V\u00e9rifiez le bon d\u00e9roulement des ex\u00e9cutions</li> <li></li> </ul> </li> </ol>"},{"location":"Cr%C3%A9ation_DAG_principal/","title":"\ud83d\ude80 \u00c9tape 3 : Cr\u00e9ation d'un pipeline ETL avec Airflow","text":""},{"location":"Cr%C3%A9ation_DAG_principal/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cr\u00e9er un pipeline ETL (Extract, Transform, Load) avec Airflow pour :</p> <ol> <li>Extraire des donn\u00e9es de ventes de magasins en France et aux \u00c9tats-Unis.</li> <li>Transformer les donn\u00e9es en convertissant les prix en GBP.</li> <li>Charger les donn\u00e9es transform\u00e9es dans un fichier CSV.</li> <li>G\u00e9n\u00e9rer un rapport consolid\u00e9.</li> </ol>"},{"location":"Cr%C3%A9ation_DAG_principal/#ressources","title":"\ud83d\udcda Ressources","text":"<ul> <li>Documentation officielle d'Airflow</li> <li>PythonOperator</li> <li>Pandas Documentation</li> </ul>"},{"location":"Cr%C3%A9ation_DAG_principal/#etapes-du-td","title":"\ud83d\udcdd \u00c9tapes du TD","text":""},{"location":"Cr%C3%A9ation_DAG_principal/#exercice-1-mise-en-place-du-pipeline-de-la-france","title":"Exercice 1 : Mise en place du pipeline de la France","text":""},{"location":"Cr%C3%A9ation_DAG_principal/#partie-1-extraction-des-donnees-pour-la-france","title":"Partie 1 : Extraction des donn\u00e9es pour la France","text":"Astuce <ul> <li>Pensez \u00e0 bien initialiser le r\u00e9pertoire de donn\u00e9es avec <code>os.makedirs()</code>.</li> <li>Les <code>default_args</code> sont essentiels pour la configuration du DAG.</li> <li>Utilisez <code>datetime.now()</code> pour la date de d\u00e9but.</li> </ul> Code initial <pre><code>from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\nimport random\nimport pandas as pd\nimport os\n\n# Configuration\nAIRFLOW_HOME = os.getenv('AIRFLOW_HOME', '/opt/airflow')\nDATA_DIR = os.path.join(AIRFLOW_HOME, 'dags')\nCSV_FILE = os.path.join(DATA_DIR, 'vente.csv')\nREPORT_FILE = os.path.join(DATA_DIR, 'rapport_ventes.txt')\n\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Taux de conversion (simul\u00e9)\nTAUX_CONVERSION = {\n    'EUR_TO_GBP': 0.85,  # 1 EUR = 0.85 GBP\n    'USD_TO_GBP': 0.79   # 1 USD = 0.79 GBP\n}\n\n# Donn\u00e9es des magasins avec prix en devise locale\nmagasins = {\n    \"usa\": {\n        \"pays\": \"\u00c9tats-Unis\",\n        \"devise\": \"USD\",\n        \"villes\": [\"New York\", \"Los Angeles\"],\n        \"noms_magasin\": [\"SuperMart USA\", \"QuickShop USA\"],\n        \"produits\": {\n            \"Pommes\": 1.80,  # Prix en USD\n            \"Bananes\": 0.95,\n            \"Lait\": 2.40,\n            \"Pain\": 1.45,\n            \"\u0152ufs\": 3.00\n        },\n        \"vendeurs\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"]\n    },\n    \"france\": {\n        \"pays\": \"France\",\n        \"devise\": \"EUR\",\n        \"villes\": [\"Paris\", \"Lyon\"],\n        \"noms_magasin\": [\"SuperMart France\", \"QuickShop France\"],\n        \"produits\": {\n            \"Pommes\": 1.30,  # Prix en EUR\n            \"Bananes\": 0.70,\n            \"Lait\": 1.80,\n            \"Pain\": 1.00,\n            \"\u0152ufs\": 2.20\n        },\n        \"vendeurs\": [\"Jean\", \"Marie\", \"Pierre\", \"Sophie\", \"Luc\"]\n    }\n}\n\ndef extraction_ventes(magasin_key):\n    \"\"\"\n    Extrait les donn\u00e9es de vente pour un magasin.\n    \"\"\"\n    magasin = magasins[magasin_key]\n    ventes = []\n\n    for produit, prix_unitaire in magasin[\"produits\"].items():\n        for vendeur in magasin[\"vendeurs\"]:\n            ville = random.choice(magasin[\"villes\"])\n            nom_magasin = random.choice(magasin[\"noms_magasin\"])\n            quantite_vendue = random.randint(1, 10)\n            prix_total = quantite_vendue * prix_unitaire\n\n            ventes.append({\n                \"pays\": magasin[\"pays\"],\n                \"devise_origine\": magasin[\"devise\"],\n                \"ville\": ville,\n                \"nom_magasin\": nom_magasin,\n                \"produit\": produit,\n                \"prix_unitaire_original\": prix_unitaire,\n                \"vendeur\": vendeur,\n                \"quantite_vendue\": quantite_vendue,\n                \"prix_total_original\": prix_total,\n                \"date_vente\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            })\n\n    return ventes\n\ndef extract_france():\n    \"\"\"Extraction des donn\u00e9es France\"\"\"\n    return extraction_ventes(\"france\")\n\n# Configuration du DAG\ndefault_args = {\n # \u00c0 compl\u00e9ter\n}\n\n# Cr\u00e9ation du DAG\ndag = DAG(\n # \u00c0 compl\u00e9ter\n)\n\n# T\u00e2ches d'extraction\nextract_france_task = PythonOperator(\n # \u00c0 compl\u00e9ter\n)\n</code></pre> <ol> <li> <p>Cr\u00e9ez un fichier <code>etl_ventes_dag.py</code> dans le dossier <code>dags</code> d'Airflow.</p> </li> <li> <p>Compl\u00e9tez le dictionnaire <code>default_args</code> avec :</p> <ul> <li><code>owner</code>: votre nom</li> <li><code>retries</code>: 2</li> <li><code>retry_delay</code>: 10 minutes</li> <li><code>start_date</code>: date actuelle</li> </ul> </li> <li> <p>Compl\u00e9tez le code de l'instance DAG avec :</p> <ul> <li>ID: <code>etl_ventes_pipeline</code></li> <li>Arguments par d\u00e9faut: <code>default_args=default_args</code></li> <li>Description personnalis\u00e9e</li> <li>Intervalle d'ex\u00e9cution : 5 minutes</li> </ul> </li> <li> <p>D\u00e9finissez la t\u00e2che <code>extract_france_task</code> avec :</p> <ul> <li><code>task_id='extract_france'</code></li> <li><code>python_callable=extract_france</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>Lancez le DAG <code>etl_ventes_pipeline</code>, double-cliquez sur <code>extract_france_task</code> et v\u00e9rifiez les donn\u00e9es extraites dans l'onglet XCom.</p> Solution compl\u00e8te <pre><code># Configuration du DAG\ndefault_args = {\n    'owner': 'votre_nom',\n    'depends_on_past': False,\n    'start_date': datetime(2025, 2, 25),\n    'retries': 2,\n    'retry_delay': timedelta(minutes=10),\n}\n\n# Cr\u00e9ation du DAG\ndag = DAG(\n    'etl_ventes_pipeline',\n    default_args=default_args,\n    description='Pipeline ETL pour les donn\u00e9es de vente',\n    schedule_interval=timedelta(minutes=5),\n    catchup=False,\n)\n\n# T\u00e2che d'extraction France\nextract_france_task = PythonOperator(\n    task_id='extract_france',\n    python_callable=extract_france,\n    dag=dag,\n)\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#partie-2-transformation-des-donnees-pour-la-france","title":"Partie 2 : Transformation des donn\u00e9es pour la France","text":"Astuce <ul> <li>Utilisez la fonction <code>xcom_pull()</code> pour r\u00e9cup\u00e9rer les donn\u00e9es de la t\u00e2che pr\u00e9c\u00e9dente.</li> <li>N'oubliez pas d'activer <code>provide_context=True</code> pour acc\u00e9der aux XComs.</li> <li>Les op\u00e9rateurs de d\u00e9pendance <code>&gt;&gt;</code> ou <code>&lt;&lt;</code> d\u00e9finissent l'ordre d'ex\u00e9cution.</li> </ul> Code initial <pre><code>def transformation_ventes(ventes, pays):\n    \"\"\"\n    Transforme les donn\u00e9es de vente en convertissant les prix en GBP.\n    \"\"\"\n    ventes_transformees = []\n    for vente in ventes:\n        vente_transformee = vente.copy()     \n        # S\u00e9lectionner le taux de conversion appropri\u00e9\n        taux = (TAUX_CONVERSION['EUR_TO_GBP'] \n                if vente['devise_origine'] == 'EUR' \n                else TAUX_CONVERSION['USD_TO_GBP'])\n        # Convertir les prix en GBP\n        vente_transformee['prix_unitaire_gbp'] = round(vente['prix_unitaire_original'] * taux, 2)\n        vente_transformee['prix_total_gbp'] = round(vente['prix_total_original'] * taux, 2)\n        vente_transformee['taux_conversion'] = taux\n        vente_transformee['date_transformation'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        ventes_transformees.append(vente_transformee)\n    print(f\"\u2713 Transformation des donn\u00e9es de {pays} termin\u00e9e\")\n    return ventes_transformees\n\ndef transform_france(**context):\n    \"\"\"Transformation des donn\u00e9es France\"\"\"\n    pass\n\ntransform_france_task = PythonOperator(\n# \u00c0 compl\u00e9ter\n)\n</code></pre> <ol> <li> <p>Ajoutez le code de transformation dans le fichier <code>etl_ventes_dag.py</code>.</p> </li> <li> <p>Compl\u00e9tez la fonction <code>transform_france</code> en utilisant XCom :</p> </li> </ol> <p>Doc xcoms</p> <pre><code>ventes_france = context['ti'].xcom_pull(task_ids='........')\nreturn transformation_ventes(ventes_france, \"France\")\n</code></pre> <ol> <li> <p>Compl\u00e9tez la t\u00e2che <code>transform_france_task</code> avec :</p> <ul> <li><code>task_id='transform_france'</code></li> <li><code>python_callable=transform_france</code></li> <li><code>provide_context=True</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>D\u00e9finissez le flux de donn\u00e9es entre les t\u00e2ches.</p> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>V\u00e9rifiez les donn\u00e9es transform\u00e9es dans l'onglet XCom de la t\u00e2che <code>transform_france</code>.</p> Solution compl\u00e8te <pre><code>def transformation_ventes(ventes, pays):\n\"\"\"\nTransforme les donn\u00e9es de vente en convertissant les prix en GBP.\n\"\"\"\nventes_transformees = []\nfor vente in ventes:\n    vente_transformee = vente.copy()     \n    # S\u00e9lectionner le taux de conversion appropri\u00e9\n    taux = (TAUX_CONVERSION['EUR_TO_GBP'] \n            if vente['devise_origine'] == 'EUR' \n            else TAUX_CONVERSION['USD_TO_GBP'])\n    # Convertir les prix en GBP\n    vente_transformee['prix_unitaire_gbp'] = round(vente['prix_unitaire_original'] * taux, 2)\n    vente_transformee['prix_total_gbp'] = round(vente['prix_total_original'] * taux, 2)\n    vente_transformee['taux_conversion'] = taux\n    vente_transformee['date_transformation'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    ventes_transformees.append(vente_transformee)\nprint(f\"\u2713 Transformation des donn\u00e9es de {pays} termin\u00e9e\")\nreturn ventes_transformees\n\ndef transform_france(**context):\n    \"\"\"Transformation des donn\u00e9es France\"\"\"\n    ventes_france = context['task_instance'].xcom_pull(task_ids='extract_france')\n    return transformation_ventes(ventes_france, \"France\")\n\ntransform_france_task = PythonOperator(\n    task_id='transform_france',\n    python_callable=transform_france,\n    provide_context=True,\n    dag=dag,\n)\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#exercice-2-mise-en-place-du-pipeline-pour-les-usa","title":"Exercice 2 : Mise en place du pipeline pour les USA","text":""},{"location":"Cr%C3%A9ation_DAG_principal/#partie-1-extraction-des-donnees-pour-les-usa","title":"Partie 1 : Extraction des donn\u00e9es pour les USA","text":"Astuce <ul> <li>Suivez la m\u00eame structure que pour la France.</li> <li>Utilisez une fonction <code>extract_usa</code> pour simuler l'extraction des donn\u00e9es.</li> </ul> <ol> <li> <p>D\u00e9finissez la t\u00e2che <code>extract_usa_task</code>  et la  fonction  <code>extract_usa</code> avec :</p> <ul> <li><code>task_id='extract_usa'</code></li> <li><code>python_callable=extract_usa</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>V\u00e9rifiez les donn\u00e9es extraites dans l'onglet XCom de la t\u00e2che <code>extract_usa</code>.</p> Solution compl\u00e8te <pre><code>def extract_usa():\n\"\"\"Extraction des donn\u00e9es USA\"\"\"\n    return extraction_ventes(\"usa\")\n# T\u00e2ches d'extraction\nextract_usa_task = PythonOperator(\n    task_id='extract_usa',\n    python_callable=extract_usa,\n    dag=dag,\n)\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#partie-2-transformation-des-donnees-pour-les-usa","title":"Partie 2 : Transformation des donn\u00e9es pour les USA","text":"Astuce <ul> <li>Utilisez la m\u00eame logique que pour la France.</li> <li>Assurez-vous de bien r\u00e9cup\u00e9rer les donn\u00e9es via <code>xcom_pull</code>.</li> </ul> <ol> <li> <p>Ajoutez la t\u00e2che <code>transform_usa_task</code> avec :</p> <ul> <li><code>task_id='transform_usa'</code></li> <li><code>python_callable=transform_usa</code></li> <li><code>provide_context=True</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>D\u00e9finissez le flux de donn\u00e9es entre les t\u00e2ches.</p> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>V\u00e9rifiez les donn\u00e9es transform\u00e9es dans l'onglet XCom de la t\u00e2che <code>transform_usa</code>.</p> Solution compl\u00e8te <pre><code>def transform_usa(**context):\n    \"\"\"Transformation des donn\u00e9es USA\"\"\"\n    ventes_usa = context['ti'].xcom_pull(task_ids='extract_usa')\n    return {\"region\": \"USA\", \"total_ventes\": sum(ventes_usa[\"ventes\"])}\n\ntransform_usa_task = PythonOperator(\n    task_id='transform_usa',\n    python_callable=transform_usa,\n    provide_context=True,\n    dag=dag,\n)\n# D\u00e9finition du flux\nextract_usa_task &gt;&gt; transform_usa_task\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#exercice-3-chargement-des-donnees-dans-un-csv","title":"Exercice 3 : Chargement des donn\u00e9es dans un CSV","text":"Astuce <ul> <li>Utilisez <code>pandas</code> pour cr\u00e9er un DataFrame et sauvegarder les donn\u00e9es dans un fichier CSV.</li> <li>Assurez-vous que le r\u00e9pertoire de sortie existe.</li> </ul> Code initial <pre><code>def load_data(**context):\n    \"\"\"Chargement des donn\u00e9es transform\u00e9es\"\"\"\n    try:\n        # R\u00e9cup\u00e9rer les donn\u00e9es transform\u00e9es\n        ventes_usa = context['......'].xcom_pull(task_ids='......')\n        ventes_france = context['......'].xcom_pull(task_ids='......')\n\n        # Combiner les donn\u00e9es\n        toutes_ventes = ventes_usa + ventes_france\n\n        # Cr\u00e9er le DataFrame\n        df = pd.DataFrame(toutes_ventes)\n\n        # G\u00e9rer le fichier existant\n        if os.path.exists(CSV_FILE):\n            df_existant = pd.read_csv(CSV_FILE)\n            df = pd.concat([df_existant, df], ignore_index=True)\n\n        # Sauvegarder\n        df.to_csv(CSV_FILE, index=False)\n        print(f\"\u2713 Donn\u00e9es charg\u00e9es dans {CSV_FILE}\")\n        print(f\"\u2713 Nombre total d'enregistrements: {len(df)}\")\n\n    except Exception as e:\n        print(f\"\u274c Erreur lors du chargement: {str(e)}\")\n        raise\n\n# T\u00e2che de chargement\nload_task = PythonOperator(\n     # \u00c0 compl\u00e9ter\n)\n\n# D\u00e9finition du flux de donn\u00e9es\n# \u00c0 compl\u00e9ter\n</code></pre> <ol> <li>Compl\u00e9tez la fonction <code>load_data</code> en utilisant XCom</li> <li> <p>Ajoutez la t\u00e2che <code>load_data_task</code> avec :</p> <ul> <li><code>task_id='load_data'</code></li> <li><code>python_callable=load_data</code></li> <li><code>provide_context=True</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>D\u00e9finissez le flux de donn\u00e9es entre les t\u00e2ches.</p> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>V\u00e9rifiez que le fichier <code>data/ventes_transformed.csv</code> est cr\u00e9\u00e9 avec les donn\u00e9es correctes.</p> Solution compl\u00e8te <pre><code>def load_data(**context):\n    \"\"\"Chargement des donn\u00e9es transform\u00e9es\"\"\"\n    try:\n        # R\u00e9cup\u00e9rer les donn\u00e9es transform\u00e9es\n        ventes_usa = context['task_instance'].xcom_pull(task_ids='transform_usa')\n        ventes_france = context['task_instance'].xcom_pull(task_ids='transform_france')\n\n        # Combiner les donn\u00e9es\n        toutes_ventes = ventes_usa + ventes_france\n\n        # Cr\u00e9er le DataFrame\n        df = pd.DataFrame(toutes_ventes)\n\n        # G\u00e9rer le fichier existant\n        if os.path.exists(CSV_FILE):\n            df_existant = pd.read_csv(CSV_FILE)\n            df = pd.concat([df_existant, df], ignore_index=True)\n\n        # Sauvegarder\n        df.to_csv(CSV_FILE, index=False)\n        print(f\"\u2713 Donn\u00e9es charg\u00e9es dans {CSV_FILE}\")\n        print(f\"\u2713 Nombre total d'enregistrements: {len(df)}\")\n\n    except Exception as e:\n        print(f\"\u274c Erreur lors du chargement: {str(e)}\")\n        raise\n# T\u00e2che de chargement\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    provide_context=True,\n    dag=dag,\n)\n# D\u00e9finition du flux de donn\u00e9es\nextract_usa_task &gt;&gt; transform_usa_task &gt;&gt; load_task\nextract_france_task &gt;&gt; transform_france_task &gt;&gt; load_task\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#exercice-4-creation-du-rapport","title":"Exercice 4 : Cr\u00e9ation du rapport","text":"Astuce <ul> <li>Utilisez <code>pandas</code> pour g\u00e9n\u00e9rer un rapport consolid\u00e9.</li> <li>Ajoutez des calculs suppl\u00e9mentaires comme la moyenne ou le total des ventes.</li> </ul> Code initial <pre><code>def generate_report(**context):\n    \"\"\"G\u00e9n\u00e8re un rapport d\u00e9taill\u00e9 des ventes\"\"\"\n    try:\n        df = pd.read_csv(CSV_FILE)\n\n        # Analyses\n        rapport = []\n        rapport.append(\"=== RAPPORT DES VENTES ===\\n\")\n\n        # Totaux par pays\n        totaux_pays = df.groupby('pays')['prix_total_gbp'].sum()\n        rapport.append(\"\\nTotaux par pays (GBP):\")\n        for pays, total in totaux_pays.items():\n            rapport.append(f\"{pays}: \u00a3{total:.2f}\")\n\n        # Meilleurs vendeurs\n        top_vendeurs = df.groupby('vendeur')['prix_total_gbp'].sum().sort_values(ascending=False).head(3)\n        rapport.append(\"\\nTop 3 des vendeurs:\")\n        for vendeur, ventes in top_vendeurs.items():\n            rapport.append(f\"{vendeur}: \u00a3{ventes:.2f}\")\n\n        # Produits les plus vendus\n        top_produits = df.groupby('produit')['quantite_vendue'].sum().sort_values(ascending=False).head(3)\n        rapport.append(\"\\nTop 3 des produits:\")\n        for produit, quantite in top_produits.items():\n            rapport.append(f\"{produit}: {quantite} unit\u00e9s\")\n\n        rapport_final = \"\\n\".join(rapport)\n\n        # Sauvegarder le rapport\n        with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n            f.write(rapport_final)\n\n        print(f\"\u2713 Rapport g\u00e9n\u00e9r\u00e9 dans {REPORT_FILE}\")\n        return rapport_final\n\n    except Exception as e:\n        print(f\"\u274c Erreur lors de la g\u00e9n\u00e9ration du rapport: {str(e)}\")\n        raise\n</code></pre> <ol> <li> <p>Ajoutez la t\u00e2che <code>generate_report_task</code> avec :</p> <ul> <li><code>task_id='generate_report'</code></li> <li><code>python_callable=generate_report</code></li> <li><code>provide_context=True</code></li> <li><code>dag=dag</code></li> </ul> </li> <li> <p>D\u00e9finissez le flux de donn\u00e9es entre les t\u00e2ches.</p> </li> <li> <p>\ud83d\udd0d V\u00e9rification :</p> </li> </ol> <p>V\u00e9rifiez que le rapport est g\u00e9n\u00e9r\u00e9 correctement.</p> Solution compl\u00e8te <pre><code>def generate_report(**context):\n    \"\"\"G\u00e9n\u00e8re un rapport d\u00e9taill\u00e9 des ventes\"\"\"\n    try:\n        df = pd.read_csv(CSV_FILE)\n\n        # Analyses\n        rapport = []\n        rapport.append(\"=== RAPPORT DES VENTES ===\\n\")\n\n        # Totaux par pays\n        totaux_pays = df.groupby('pays')['prix_total_gbp'].sum()\n        rapport.append(\"\\nTotaux par pays (GBP):\")\n        for pays, total in totaux_pays.items():\n            rapport.append(f\"{pays}: \u00a3{total:.2f}\")\n\n        # Meilleurs vendeurs\n        top_vendeurs = df.groupby('vendeur')['prix_total_gbp'].sum().sort_values(ascending=False).head(3)\n        rapport.append(\"\\nTop 3 des vendeurs:\")\n        for vendeur, ventes in top_vendeurs.items():\n            rapport.append(f\"{vendeur}: \u00a3{ventes:.2f}\")\n\n        # Produits les plus vendus\n        top_produits = df.groupby('produit')['quantite_vendue'].sum().sort_values(ascending=False).head(3)\n        rapport.append(\"\\nTop 3 des produits:\")\n        for produit, quantite in top_produits.items():\n            rapport.append(f\"{produit}: {quantite} unit\u00e9s\")\n\n        rapport_final = \"\\n\".join(rapport)\n\n        # Sauvegarder le rapport\n        with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n            f.write(rapport_final)\n\n        print(f\"\u2713 Rapport g\u00e9n\u00e9r\u00e9 dans {REPORT_FILE}\")\n        return rapport_final\n\n    except Exception as e:\n        print(f\"\u274c Erreur lors de la g\u00e9n\u00e9ration du rapport: {str(e)}\")\n        raise\n\ngenerate_report_task = PythonOperator(\n    task_id='generate_report',\n    python_callable=generate_report,\n    provide_context=True,\n    dag=dag,\n)\n\n\n# D\u00e9finition du flux\nload_task &gt;&gt; generate_report_task\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_principal/#resultat-final","title":"\ud83c\udf89 R\u00e9sultat final","text":"<p>Vous avez maintenant un pipeline ETL complet avec Airflow qui :</p> <ol> <li> <p>Extrait les donn\u00e9es de ventes pour la France et les USA.</p> </li> <li> <p>Transforme les donn\u00e9es en calculant le total des ventes.</p> </li> <li> <p>Charge les donn\u00e9es transform\u00e9es dans un fichier CSV.</p> </li> <li> <p>G\u00e9n\u00e8re un rapport consolid\u00e9.</p> </li> </ol>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/","title":"\ud83d\ude80 \u00c9tape 4 :  Pipeline MLOps avec Airflow et Docker","text":""},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#objectif","title":"Objectif","text":"<p>L'objectif de cet exercice est de mettre en place un pipeline MLOps automatis\u00e9 avec Apache Airflow et Docker pour g\u00e9rer l'entra\u00eenement et l'\u00e9valuation d'un mod\u00e8le de classification sur le dataset Iris.</p> <p>Nous allons :</p> <ol> <li>Charger et pr\u00e9traiter les donn\u00e9es \ud83d\udcca</li> <li>Entra\u00eener un mod\u00e8le RandomForest \ud83c\udfaf</li> <li>\u00c9valuer la performance du mod\u00e8le \ud83d\udcc8</li> <li>Mettre en place un DAG Airflow pour automatiser ces \u00e9tapes</li> </ol>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#structure-du-projet","title":"\ud83d\udcc2 Structure du projet","text":"<pre><code>mlops/\n\u2502\u2500\u2500 airflow-docker/\n\u2502   \u251c\u2500\u2500 dags/                      # Contient les DAGs (workflow Airflow)\n\u2502   \u2502   \u251c\u2500\u2500 mlops_pipeline.py\n\u2502   \u2502   \u2502\u2500\u2500 src/                            # Code m\u00e9tier (pr\u00e9traitement, entra\u00eenement...)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 preprocessing.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 training.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 evaluation.py\n\u2502   \u2502   \u2502\u2500\u2500 data/                           # Datasets si besoin\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data.csv\n\u2502   \u2502   \u2502\u2500\u2500 models/                         # Stockage des mod\u00e8les entra\u00een\u00e9s\n\u2502   \u251c\u2500\u2500 plugins/                   # Plugins Airflow si n\u00e9cessaire\n\u2502   \u251c\u2500\u2500 logs/                       # Logs d'ex\u00e9cution\n\u2502   \u251c\u2500\u2500 docker-compose.yml          # D\u00e9ploiement Docker\n\u2502   \u251c\u2500\u2500 requirements.txt            # D\u00e9pendances\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#1-etape-1-pretraitement-des-donnees","title":"1\ufe0f\u20e3 \u00c9tape 1 - Pr\u00e9traitement des donn\u00e9es","text":"<p>\ud83d\udccc Objectif : Charger et nettoyer les donn\u00e9es Iris, puis les sauvegarder.</p> <p>\ud83d\udccd Fichier : <code>src/preprocessing.py</code> <pre><code>import os\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\ndef preprocess_data():\n    print(\"Pr\u00e9traitement des donn\u00e9es...\")\n\n    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../data\"))\n    os.makedirs(data_dir, exist_ok=True)\n\n    # Charger le dataset Iris\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    file_path = os.path.join(data_dir, \"iris.csv\")\n\n    df.to_csv(file_path, index=False)\n    print(f\"Donn\u00e9es sauvegard\u00e9es : {file_path}\")\n\n    return file_path\n</code></pre></p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#2-etape-2-entrainement-du-modele","title":"2\ufe0f\u20e3 \u00c9tape 2 - Entra\u00eenement du mod\u00e8le","text":"<p>\ud83d\udccc Objectif : Entra\u00eener un mod\u00e8le <code>RandomForestClassifier</code> et sauvegarder le mod\u00e8le.</p> <p>\ud83d\udccd Fichier : <code>src/training.py</code> <pre><code>import os\nimport pandas as pd\nimport joblib\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef train_model():\n    print(\"Entra\u00eenement du mod\u00e8le...\")\n\n    data= os.path.abspath(os.path.join(os.path.dirname(__file__), \"../data/iris.csv\"))\n\n    if not os.path.exists(data):\n        raise FileNotFoundError(f\"{data} introuvable. Ex\u00e9cute preprocessing.py\")\n\n    models_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../models\"))\n    os.makedirs(models_dir, exist_ok=True)  \n\n    df = pd.read_csv(data)\n    X = df.drop(columns=[\"target\"])\n    y = df[\"target\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    model_file_path = os.path.join(models_dir, \"model.pkl\")\n\n    joblib.dump(model, model_file_path)\n    print(f\"Mod\u00e8le sauvegard\u00e9 : {model_file_path}\")\n</code></pre></p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#3-etape-3-evaluation-du-modele","title":"3\ufe0f\u20e3 \u00c9tape 3 - \u00c9valuation du mod\u00e8le","text":"<p>\ud83d\udccc Objectif : Calculer la pr\u00e9cision du mod\u00e8le et sauvegarder le score.</p> <p>\ud83d\udccd Fichier : <code>src/evaluation.py</code> <pre><code>import os\nimport pandas as pd\nimport joblib\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef evaluate_model():\n    print(\"\u00c9valuation du mod\u00e8le...\")\n\n    data = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../data/iris.csv\"))\n    model = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../models/model.pkl\"))\n\n    # V\u00e9rifier si les fichiers existent\n    if not os.path.exists(data):\n        raise FileNotFoundError(f\"{data} introuvable. Ex\u00e9cuter preprocessing.py\")\n\n    if not os.path.exists(model):\n        raise FileNotFoundError(f\"{model} introuvable. Ex\u00e9cuter train_model.py\")\n\n    # Charger les donn\u00e9es\n    df = pd.read_csv(data)\n    X = df.drop(columns=[\"target\"])\n    y = df[\"target\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = joblib.load(model)\n\n    y_pred = model.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Pr\u00e9cision du mod\u00e8le : {accuracy:.4f}\")\n\n    return accuracy\n</code></pre></p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#4-etape-4-dag-airflow","title":"4\ufe0f\u20e3 \u00c9tape 4 - DAG Airflow","text":"<p>\ud83d\udccd Fichier : <code>dags/mlops_pipeline.py</code> <pre><code>import logging\nfrom datetime import datetime, timedelta\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import PythonOperator\n\nfrom src.preprocessing import preprocess_data\nfrom src.training import train_model\nfrom src.evaluation import evaluate_model\n\n\ndefault_args = {\n    'owner': 'mlops-airflow',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=10),\n    'start_date': datetime.now(),\n}\n\ndag = DAG('mlops_pipeline',\n    # \u00c0 compl\u00e9ter\n)\n\ntask_1 = PythonOperator(\n    # \u00c0 compl\u00e9ter\n)\n\ntask_2 = PythonOperator(\n    # \u00c0 compl\u00e9ter\n)\n\ntask_3 = PythonOperator(\n    # \u00c0 compl\u00e9ter\n)\n\ntask_1 &gt;&gt; # \u00c0 compl\u00e9ter \ntask_1 &lt;&lt; # \u00c0 compl\u00e9ter\n</code></pre></p> Afficher la solution <pre><code>import logging\nfrom datetime import datetime, timedelta\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import PythonOperator\n\nfrom src.preprocessing import preprocess_data\nfrom src.training import train_model\nfrom src.evaluation import evaluate_model\n\n\ndefault_args = {\n    'owner': 'mlops-airflow',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=10),\n    'start_date': datetime.now(),\n}\n\ndag = DAG('mlops_pipeline',\n        default_args=default_args,\n        schedule_interval=timedelta(days=1),\n        catchup=False\n)\n\ntask_1 = PythonOperator(\n    task_id='preprocess_data',\n    python_callable=preprocess_data,\n    dag=dag\n)\n\ntask_2 = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model,\n    dag=dag\n)\n\ntask_3 = PythonOperator(\n    task_id='evaluate_model',\n    python_callable=evaluate_model,\n    dag=dag\n)\n\ntask_1 &gt;&gt; task_2 &gt;&gt; task_3\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#5-lancer-le-pipeline","title":"5\ufe0f\u20e3 Lancer le pipeline","text":"<p>1\ufe0f\u20e3 D\u00e9marrer Airflow avec Docker</p> <p>Modifier la ligne 71 du <code>docker-compose.yml</code> pour ajouter les d\u00e9pendances : :</p> <p> <pre><code>    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- pandas requests scikit-learn numpy logging}\n</code></pre> </p> <pre><code>docker-compose up\n</code></pre> <p>2\ufe0f\u20e3 Acc\u00e9der \u00e0 Airflow</p> <ul> <li>Ouvrir http://localhost:8080</li> <li>Activer et ex\u00e9cuter le DAG <code>mlops_pipeline</code></li> </ul>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#6-generation-du-rapport","title":"6\ufe0f\u20e3 G\u00e9n\u00e9ration du rapport","text":"<p>Maintenant qu'on a <code>accuracy</code>, nous allons g\u00e9n\u00e9rer un rapport contenant cette m\u00e9trique.</p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#objectif-generer-un-rapport-avec-laccuracy","title":"\ud83d\udccc Objectif : G\u00e9n\u00e9rer un rapport avec l'accuracy","text":"<p>\ud83d\udccd Fichier : <code>src/generate_report.py</code></p> <p><pre><code>import os\n\ndef generate_report(**kwargs):\n    ti = kwargs['ti']  # R\u00e9cup\u00e9rer l'accuracy depuis XCom\n    accuracy = ti.xcom_pull(task_ids='evaluate_model', key='accuracy')\n\n    if accuracy is None:\n        raise ValueError(\"L'accuracy n'a pas \u00e9t\u00e9 trouv\u00e9e dans XCom.\")\n\n    report_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../report\"))\n    os.makedirs(report_dir, exist_ok=True)\n\n    report_content = f\"\"\"\n    # Rapport d'\u00c9valuation du Mod\u00e8le\n\n    **Accuracy**: {accuracy}\n\n    **Commentaire**: Le mod\u00e8le a atteint une accuracy de {accuracy}.\n    \"\"\"\n\n    report_path = os.path.join(report_dir, \"model_accuracy_report.md\")\n    with open(report_path, \"w\") as file:\n        file.write(report_content)\n\n    return report_path\n</code></pre> Nous avons ajout\u00e9 la g\u00e9n\u00e9ration du rapport, nous devons mettre \u00e0 jour le DAG pour inclure cette nouvelle t\u00e2che.</p> <p>\ud83d\udccd Fichier : <code>mlops_pipeline.py</code></p> <pre><code>from src.generate_report import generate_report\n\ntask_4 = PythonOperator(\n    # \u00c0 compl\u00e9ter\n)\n\ntask_3 &gt;&gt; task_4\n</code></pre> Solution compl\u00e8te <pre><code>from src.generate_report import generate_report\n\ntask_4 = PythonOperator(\n    task_id='generate_report',\n    python_callable=generate_report,\n    provide_context=True,\n    dag=dag\n)\n\ntask_3 &gt;&gt; task_4\n</code></pre> <p>Pour s'assurer <code>evaluate_model</code>  envoie correctement l'accuracy et que <code>task_3</code> r\u00e9cup\u00e8re bien la valeur transmise, on doit faire certaine modification.</p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#modification-de-evaluate_modelpy-pour-envoyer-correctement-laccuracy","title":"\ud83d\udccc Modification de <code>evaluate_model.py</code> pour envoyer correctement l'accuracy","text":"<p>Dans <code>src/evaluation.py</code>, nous devons nous assurer que l'accuracy est bien stock\u00e9e dans XCom et r\u00e9cup\u00e9rable par les t\u00e2ches suivantes.</p> <p>\ud83d\udccd Fichier : <code>src/evaluation.py</code> <pre><code>import os\nimport pandas as pd\nimport joblib\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef evaluate_model(**kwargs):\n    print(\"\u00c9valuation du mod\u00e8le...\")\n\n    data = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../data/iris.csv\"))\n    model = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../models/model.pkl\"))\n\n    # V\u00e9rifier si les fichiers existent\n    if not os.path.exists(data):\n        raise FileNotFoundError(f\"{data} introuvable. Ex\u00e9cuter preprocessing.py\")\n\n    if not os.path.exists(model):\n        raise FileNotFoundError(f\"{model} introuvable. Ex\u00e9cuter train_model.py\")\n\n    # Charger les donn\u00e9es\n    df = pd.read_csv(data)\n    X = df.drop(columns=[\"target\"])\n    y = df[\"target\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = joblib.load(model)\n\n    y_pred = model.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Pr\u00e9cision du mod\u00e8le : {accuracy:.4f}\")\n\n    ti = kwargs['ti']  \n    ti.xcom_push(key='accuracy', value=accuracy)\n\n    return accuracy\n</code></pre></p>"},{"location":"Cr%C3%A9ation_DAG_w_Pipeline_MLOps/#modifier-task_3-dans-mlops_pipelinepy-pour-recevoir-les-donnees-correctement","title":"\ud83d\udccc Modifier <code>task_3</code> dans <code>mlops_pipeline.py</code> pour recevoir les donn\u00e9es correctement","text":"<p>Dans <code>mlops_pipeline.py</code>, nous devons nous assurer que <code>task_3</code> (\u00e9valuation) est bien configur\u00e9 pour recevoir et transmettre les donn\u00e9es.</p> <p>\ud83d\udccd Fichier : <code>mlops_pipeline.py</code></p> <pre><code>from src.evaluation import evaluate_model\n\ntask_3 = PythonOperator(\n    task_id='evaluate_model',\n    # \u00c0 compl\u00e9ter\n)\n</code></pre> Solution compl\u00e8te <pre><code>from src.evaluation import evaluate_model\n\ntask_3 = PythonOperator(\n    task_id='evaluate_model',\n    python_callable=evaluate_model,\n    provide_context=True,  # \u2705 Assurer le passage des donn\u00e9es via XCom\n    dag=dag\n)\n</code></pre> <p>Une fois ces modifications effectu\u00e9es, <code>evaluate_model</code> enverra correctement l'accuracy, et <code>task_3</code> r\u00e9cup\u00e9rera et transmettra bien les donn\u00e9es pour <code>generate_report</code>.</p>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/","title":"\ud83d\ude80 \u00c9tape 5 : de fichiers et traitement avec Airflow","text":""},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cr\u00e9er un DAG Airflow qui surveille un dossier pour d\u00e9tecter l'arriv\u00e9e d'un fichier CSV, charge son contenu dans une base de donn\u00e9es SQLite, puis archive le fichier trait\u00e9. Ce TD vous permettra de comprendre comment utiliser les op\u00e9rateurs FileSensor, PythonOperator, et BashOperator pour cr\u00e9er un pipeline de traitement de fichiers.</p>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#ressources","title":"\ud83d\udcda Ressources","text":"<ul> <li>Documentation officielle d'Airflow</li> <li>FileSensor Documentation</li> <li>PythonOperator Documentation</li> <li>BashOperator Documentation</li> <li>data</li> <li>csv</li> </ul>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#etapes-du-td","title":"\ud83d\udcdd \u00c9tapes du TD","text":""},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#exercice-1-configuration-initiale","title":"Exercice 1 : Configuration initiale","text":""},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#description","title":"Description","text":"<p>Nous allons configurer un DAG pour surveiller un dossier, traiter un fichier CSV, et archiver le fichier apr\u00e8s traitement.</p>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#taches-a-realiser","title":"T\u00e2ches \u00e0 r\u00e9aliser","text":"<ol> <li> <p>Configurer le volume partag\u00e9 :</p> <ul> <li>Ajoutez le chemin du dossier \u00e0 surveiller dans les volumes de <code>airflow-common-env</code> :     <pre><code>- C:\\Users\\Joel\\Documents\\Python\\test_airflow:/appdata\n</code></pre></li> </ul> </li> <li> <p>Cr\u00e9er la connexion FileSensor :</p> </li> <li> <p>Cr\u00e9er le DAG :</p> <ul> <li>Cr\u00e9ez un fichier <code>file_processing_dag.py</code> dans le dossier <code>dags</code>.</li> </ul> </li> </ol>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#-via-la-ligne-de-commande","title":"- Via la ligne de commande :","text":"<pre><code>```bash\nairflow connections add 'file_sensor_conn' --conn-type 'fs' --conn-extra '{\"path\": \"/appdata\"}'\n```\n</code></pre> <ul> <li>Via l'interface utilisateur (GUI) :<ul> <li>Allez dans <code>Admin &gt;&gt; Connections &gt;&gt; +</code></li> <li>Remplissez les champs :</li> <li>Conn Id</li> <li>Conn Type</li> </ul> </li> </ul>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#exercice-2-creation-du-dag","title":"Exercice 2 : Cr\u00e9ation du DAG","text":""},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#description_1","title":"Description","text":"<p>Nous allons cr\u00e9er un DAG avec les t\u00e2ches suivantes :</p> <ul> <li> <p>FileSensor : Surveiller l'arriv\u00e9e d'un fichier CSV.  </p> <pre><code>-task_id\n-filepath\n-fs_conn_id\n-timeout\n-poke_interval\n-mode\n</code></pre> </li> <li> <p>PythonOperator : Charger le fichier CSV dans une base de donn\u00e9es SQLite.</p> </li> <li>BashOperator : Archiver le fichier trait\u00e9.</li> <li>PythonOperator : Afficher quelques lignes de la table pour v\u00e9rifier l'importation.</li> </ul>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#taches-a-realiser_1","title":"T\u00e2ches \u00e0 r\u00e9aliser","text":"Code initial <pre><code>import os\nimport glob\nimport pandas as pd\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom airflow import DAG, Dataset\nfrom airflow.operators.python import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.operators.bash import BashOperator\nimport logging\n\nmyfile = Dataset(\"file:///appdata/database/stores.db\")\n\n# Define the SQLite database path\nDB_PATH = '/appdata/database/stores.db'  # Change this to your SQLite database path\nDATA_FOLDER = '/appdata/data'  # Change this to your data folder path\n\ndef load_csv_to_sqlite():\n    # # Insert data into sales with corresponding store_id\n    sales = pd.read_csv(\"/appdata/data/sales_2010.csv\")\n    conn = sqlite3.connect(DB_PATH)\n    conn.execute(\"PRAGMA foreign_keys = ON\")  # Enable foreign key support\n    for _, row in sales.iterrows():\n        conn.execute('''\n        INSERT INTO sales (store_id, Dept, Date, Weekly_Sales, IsHoliday) VALUES (?, ?, ?, ?, ?)\n        ''', (row[\"Store\"], row['Dept'], row['Date'], row['Weekly_Sales'], row['IsHoliday']))\n    conn.commit()\n    conn.close()\n    print(sales.head(10))\n\ndef query_db():\n    # Querying the SQLite database to check the data\n    conn = sqlite3.connect(DB_PATH)\n    sales_query = pd.read_sql_query(\"SELECT * FROM sales\", conn)\n    print(\"\\nSales:\")\n    print(sales_query)\n\n    conn.close()\n</code></pre> <ol> <li> <p>D\u00e9finir les arguments par d\u00e9faut :    <pre><code> # Default arguments for the DAG\n default_args = {\n     'owner': 'joel',\n     'retries': 1,\n     'retry_delay': timedelta(minutes=5),\n     'start_date': datetime(2023, 1, 1),  # Adjust this date\n }\n</code></pre></p> </li> <li> <p>Cr\u00e9er l'instance du DAG :    <pre><code> # Create the DAG\n with DAG(\n     dag_id='filesensor_dag3',\n     default_args=default_args,\n     schedule_interval=timedelta(days=1),  # Adjust the schedule as needed\n     catchup=False,\n ) as dag:\n</code></pre></p> </li> <li> <p>Ajouter la t\u00e2che FileSensor :    <pre><code> # Define the File Sensor to wait for new CSV files\n wait_for_csv = FileSensor(\n     task_id='wait_for_csv',\n     filepath='/appdata/data/*.csv',  # Detect any CSV file in the folder\n     fs_conn_id='data_folder',  # Connection ID for filesystem\n     timeout=60 *2,  # Timeout after 10 minutes\n     poke_interval=10,  # Check every 30 seconds\n     mode='poke',  # Use poke mode to wait for the file\n )\n</code></pre></p> </li> <li> <p>Ajouter la t\u00e2che PythonOperator pour charger le fichier CSV :    <pre><code>  # Define the task to load CSV files into SQLite\n  load_task = PythonOperator(\n      task_id='load_csv_to_sqlite',\n      python_callable=load_csv_to_sqlite,\n      provide_context=True,\n      outlets=[myfile]\n  )\n</code></pre></p> </li> <li> <p>Ajouter la t\u00e2che BashOperator pour archiver le fichier :    <pre><code>  archive_csv_task = BashOperator(\n   task_id=\"archive_csv\",\n   bash_command=\"mv /appdata/data/* /appdata/archive/\"\n )\n</code></pre></p> </li> <li> <p>D\u00e9finir les d\u00e9pendances entre les t\u00e2ches :    <pre><code> # Set task dependencies\n wait_for_csv &gt;&gt; load_task &gt;&gt; archive_csv_task\n</code></pre></p> </li> <li> <p>Ajouter la t\u00e2che PythonOperator pour afficher les donn\u00e9es :</p> </li> </ol> Code initial <pre><code>    from airflow import DAG, Dataset\n    from airflow.decorators import task\n    from airflow.operators.python import PythonOperator\n\n    import sqlite3\n    import pandas as pd\n    from datetime import datetime, timedelta\n\n    # myfile = Dataset(\"file:///appdata/archive/sales.csv\")\n    myfile = Dataset(\"file:///appdata/database/stores.db\")\n\n    default_args = {\n        'owner': 'joel',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5),\n        'start_date': datetime(2023, 1, 1),  # Adjust this date\n    }\n\n    def count_sales_rows():\n        \"\"\"Query SQLite database to count rows in sales table.\"\"\"\n        db_path = \"/appdata/database/stores.db\"  # Update if necessary\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT COUNT(*) FROM sales\")\n        row_count = cursor.fetchone()[0]  # Fetch the count result\n\n        conn.close()\n        print(f\"Total rows in sales table: {row_count}\")  # Log the result\n\n    with DAG(\n        dag_id=\"dataset_consumer2\",\n        default_args=default_args,\n        schedule=[myfile],\n        catchup=False\n    ) as dag:\n\n        query_db_task = PythonOperator(\n            task_id=\"query_db\",\n            python_callable=count_sales_rows\n        )\n\n        query_db_task\n</code></pre>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#verification","title":"\ud83d\udd0d V\u00e9rification","text":"<p>Pour valider votre DAG :</p> <ol> <li>V\u00e9rifiez la pr\u00e9sence du DAG dans l'interface web d'Airflow.</li> <li>D\u00e9clenchez manuellement le DAG via l'interface.</li> <li>Analysez les logs pour chaque t\u00e2che pour confirmer leur bon d\u00e9roulement.</li> <li>V\u00e9rifiez les r\u00e9sultats :<ul> <li>Le fichier CSV a \u00e9t\u00e9 charg\u00e9 dans la base de donn\u00e9es.</li> <li>Le fichier a \u00e9t\u00e9 archiv\u00e9.</li> <li>Les donn\u00e9es ont \u00e9t\u00e9 affich\u00e9es correctement.</li> </ul> </li> </ol>"},{"location":"Cr%C3%A9ation_DAG_ww_Surveillance/#solution-complete","title":"Solution compl\u00e8te","text":"Afficher la solution <pre><code>import os\nimport glob\nimport pandas as pd\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom airflow import DAG, Dataset\nfrom airflow.operators.python import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.operators.bash import BashOperator\nimport logging\n\nmyfile = Dataset(\"file:///appdata/database/stores.db\")\n\n# Define the SQLite database path\nDB_PATH = '/appdata/database/stores.db'  # Change this to your SQLite database path\nDATA_FOLDER = '/appdata/data'  # Change this to your data folder path\n\ndef load_csv_to_sqlite():\n    # # Insert data into sales with corresponding store_id\n    sales = pd.read_csv(\"/appdata/data/sales_2010.csv\")\n    conn = sqlite3.connect(DB_PATH)\n    conn.execute(\"PRAGMA foreign_keys = ON\")  # Enable foreign key support\n    for _, row in sales.iterrows():\n        conn.execute('''\n        INSERT INTO sales (store_id, Dept, Date, Weekly_Sales, IsHoliday) VALUES (?, ?, ?, ?, ?)\n        ''', (row[\"Store\"], row['Dept'], row['Date'], row['Weekly_Sales'], row['IsHoliday']))\n    conn.commit()\n    conn.close()\n    print(sales.head(10))\n\ndef query_db():\n    # Querying the SQLite database to check the data\n    conn = sqlite3.connect(DB_PATH)\n    sales_query = pd.read_sql_query(\"SELECT * FROM sales\", conn)\n    print(\"\\nSales:\")\n    print(sales_query)\n\n    conn.close()\n\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'joel',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    'start_date': datetime(2023, 1, 1),  # Adjust this date\n}\n\n# Create the DAG\nwith DAG(\n    dag_id='filesensor_dag3',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),  # Adjust the schedule as needed\n    catchup=False,\n) as dag:\n\n\n    # Define the File Sensor to wait for new CSV files\n    wait_for_csv = FileSensor(\n        task_id='wait_for_csv',\n        filepath='/appdata/data/*.csv',  # Detect any CSV file in the folder\n        fs_conn_id='data_folder',  # Connection ID for filesystem\n        timeout=60 *2,  # Timeout after 10 minutes\n        poke_interval=10,  # Check every 30 seconds\n        mode='poke',  # Use poke mode to wait for the file\n    )\n\n    # Define the task to load CSV files into SQLite\n    load_task = PythonOperator(\n        task_id='load_csv_to_sqlite',\n        python_callable=load_csv_to_sqlite,\n        provide_context=True,\n        outlets=[myfile]\n    )\n\n    archive_csv_task = BashOperator(\n        task_id=\"archive_csv\",\n        bash_command=\"mv /appdata/data/* /appdata/archive/\"\n    )\n\n\n    # Set task dependencies\n    wait_for_csv &gt;&gt; load_task &gt;&gt; archive_csv_task\n</code></pre>"},{"location":"WWW/","title":"\ud83c\udd98 Aide \u00e0 l'installation de Docker et gestion des erreurs de modules dans Apache Airflow","text":""},{"location":"WWW/#la-methode-la-plus-simple","title":"\ud83d\udccc La m\u00e9thode la plus simple","text":"<p>La fa\u00e7on la plus simple d\u2019ajouter vos d\u00e9pendances est de modifier le fichier <code>docker-compose.yaml</code>. </p> <ul> <li>\u00c9tape 1 : Rendez-vous \u00e0 la ligne 71 du fichier <code>docker-compose.yaml</code>.</li> <li>\u00c9tape 2 : Ajoutez tous les modules Python dont vous avez besoin \u00e0 la variable <code>_PIP_ADDITIONAL_REQUIREMENTS</code>.</li> </ul> <p>Cependant, pour une gestion plus propre et plus professionnelle des d\u00e9pendances, il est recommand\u00e9 d\u2019adopter une approche plus robuste en construisant une image Docker personnalis\u00e9e.</p> <p>\ud83d\udd17 Pour en savoir plus sur la bonne mani\u00e8re de g\u00e9rer les d\u00e9pendances, consultez la documentation officielle d'Airflow : [Airflow Docker Customization][def]</p>"},{"location":"WWW/#installation-propre-des-dependances-avec-dockerfile","title":"\ud83d\ude80 Installation propre des d\u00e9pendances avec Dockerfile","text":""},{"location":"WWW/#etape-1-creer-un-dockerfile","title":"\u00c9tape 1 : Cr\u00e9er un Dockerfile","text":"<p>Afin de pouvoir installer les modules n\u00e9cessaires via un fichier <code>requirements.txt</code>, commencez par cr\u00e9er un <code>Dockerfile</code> dans le m\u00eame dossier que <code>docker-compose.yaml</code>.</p> <p>Assurez-vous \u00e9galement d\u2019avoir un fichier <code>requirements.txt</code> listant vos d\u00e9pendances.</p>"},{"location":"WWW/#attention-aux-dependances-specifiques","title":"\u26a0\ufe0f Attention aux d\u00e9pendances sp\u00e9cifiques","text":"<p>Certaines biblioth\u00e8ques n\u00e9cessitent des d\u00e9pendances syst\u00e8me sp\u00e9cifiques.  Par exemple, <code>scikit-learn</code> a besoin de <code>gcc</code>, <code>g++</code>, <code>make</code> et <code>python3-dev</code> pour \u00eatre compil\u00e9 correctement.</p> <p>Ajoutez donc ces d\u00e9pendances dans le <code>Dockerfile</code> avant d'installer <code>requirements.txt</code> :</p> <pre><code>FROM apache/airflow:2.10.5\n\nUSER root\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc g++ make python3-dev\n\nUSER airflow\n\nCOPY requirements.txt /requirements.txt\n\nRUN pip install --no-cache-dir \"apache-airflow==${AIRFLOW_VERSION}\" -r /requirements.txt\n</code></pre>"},{"location":"WWW/#etape-2-modifier-le-docker-composeyaml","title":"\u00c9tape 2 : Modifier le <code>docker-compose.yaml</code>","text":"<p>Dans le fichier <code>docker-compose.yaml</code> : - Commentez la ligne 52 :   <pre><code># image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.5}\n</code></pre> - D\u00e9commentez la ligne 53 pour utiliser le Dockerfile que nous avons cr\u00e9\u00e9 :   <pre><code>build: .\n</code></pre></p>"},{"location":"WWW/#etape-3-construire-et-lancer-les-conteneurs","title":"\u00c9tape 3 : Construire et lancer les conteneurs","text":"<p>Ex\u00e9cutez les commandes suivantes pour reconstruire et d\u00e9marrer l\u2019application : <pre><code>docker-compose build\ndocker-compose up -d\n</code></pre></p> <p>Avec cette approche, votre environnement Airflow est bien configur\u00e9 et pr\u00eat \u00e0 ex\u00e9cuter des t\u00e2ches avec toutes les d\u00e9pendances requises. \u2705</p> <p>\ud83c\udfaf F\u00e9licitations ! Tu as maintenant un pipeline MLOps complet avec Airflow pour entra\u00eener, \u00e9valuer et automatiser un mod\u00e8le de classification Iris ! \ud83d\ude80</p>"}]}