# üöÄ TD : Surveillance de fichiers et traitement avec Airflow

## üéØ Objectif
Cr√©er un DAG Airflow qui surveille un dossier pour d√©tecter l'arriv√©e d'un fichier CSV, charge son contenu dans une base de donn√©es SQLite, puis archive le fichier trait√©.

## üìö Ressources
- [Documentation officielle d'Airflow](https://airflow.apache.org/docs/apache-airflow/stable/index.html)
- [FileSensor Documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/filesystem/index.html)
- [PythonOperator Documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)
- [BashOperator Documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html)

---

## üìù Exercices Pratiques

### Exercice 1 : Configuration initiale

#### Description
Nous allons configurer un DAG pour surveiller un dossier, traiter un fichier CSV, et archiver le fichier apr√®s traitement.

#### T√¢ches √† r√©aliser
1. **Configurer le volume partag√©** :
   - Ajoutez le chemin du dossier √† surveiller dans les volumes de `airflow-common-env` :
     ```yaml
     - C:\Users\Joel\Documents\Python\test_airflow:/appdata
     ```

2. **Cr√©er la connexion FileSensor** :
   - Via la ligne de commande :
     ```bash
     airflow connections add 'file_sensor_conn' --conn-type 'fs' --conn-extra '{"path": "/appdata"}'
     ```
   - Via l'interface utilisateur (GUI) :
     - Allez dans `Admin >> Connections >> +`
     - Remplissez les champs :
       - Conn Id: `file_sensor_conn`
       - Conn Type: `File (path)`
       - Extra: `{"path": "/appdata"}`

3. **Cr√©er le DAG** :
   - Cr√©ez un fichier `file_processing_dag.py` dans le dossier `dags`.

---

### Exercice 2 : Cr√©ation du DAG

#### Description
Nous allons cr√©er un DAG avec les t√¢ches suivantes :
1. **FileSensor** : Surveiller l'arriv√©e d'un fichier CSV.
2. **PythonOperator** : Charger le fichier CSV dans une base de donn√©es SQLite.
3. **BashOperator** : Archiver le fichier trait√©.
4. **PythonOperator** : Afficher quelques lignes de la table pour v√©rifier l'importation.

#### T√¢ches √† r√©aliser

1. **Importer les modules n√©cessaires** :
   ```python
   from datetime import datetime, timedelta
   from airflow.models.dag import DAG
   from airflow.sensors.filesystem import FileSensor
   from airflow.operators.python import PythonOperator
   from airflow.operators.bash import BashOperator
   import pandas as pd
   import sqlite3
   import os
   ```

2. **D√©finir les arguments par d√©faut** :
   ```python
   default_args = {
       'owner': 'votre_nom',
       'retries': 2,
       'retry_delay': timedelta(minutes=5),
       'start_date': datetime(2023, 10, 1),
   }
   ```

3. **Cr√©er l'instance du DAG** :
   ```python
   dag = DAG(
       'file_processing_dag',
       default_args=default_args,
       description='DAG pour surveiller et traiter un fichier CSV',
       schedule_interval=timedelta(days=1),
       catchup=False
   )
   ```

4. **Ajouter la t√¢che FileSensor** :
   ```python
   task_file_sensor = FileSensor(
       task_id='file_sensor_task',
       filepath='data.csv',
       fs_conn_id='file_sensor_conn',
       poke_interval=30,
       timeout=300,
       mode='poke',
       dag=dag
   )
   ```

5. **Ajouter la t√¢che PythonOperator pour charger le fichier CSV** :
   ```python
   def load_csv_to_db():
       csv_path = '/appdata/data.csv'
       db_path = '/appdata/stores.db'
       df = pd.read_csv(csv_path)
       conn = sqlite3.connect(db_path)
       df.to_sql('sales', conn, if_exists='append', index=False)
       conn.close()

   task_load_csv = PythonOperator(
       task_id='load_csv_task',
       python_callable=load_csv_to_db,
       dag=dag
   )
   ```

6. **Ajouter la t√¢che BashOperator pour archiver le fichier** :
   ```python
   task_archive_file = BashOperator(
       task_id='archive_file_task',
       bash_command='mv /appdata/data.csv /appdata/archive/data.csv',
       dag=dag
   )
   ```

7. **Ajouter la t√¢che PythonOperator pour afficher les donn√©es** :
   ```python
   def display_data():
       db_path = '/appdata/stores.db'
       conn = sqlite3.connect(db_path)
       df = pd.read_sql_query('SELECT * FROM sales LIMIT 5', conn)
       print(df)
       conn.close()

   task_display_data = PythonOperator(
       task_id='display_data_task',
       python_callable=display_data,
       dag=dag
   )
   ```

8. **D√©finir les d√©pendances entre les t√¢ches** :
   ```python
   task_file_sensor >> task_load_csv >> task_archive_file >> task_display_data
   ```

---

### Exercice 3 : V√©rification et test

#### Description
V√©rifiez que le DAG fonctionne correctement en suivant les √©tapes ci-dessous.

#### T√¢ches √† r√©aliser

1. **V√©rifier la pr√©sence du DAG** :
   - Acc√©dez √† l'interface web d'Airflow : [http://localhost:8080/](http://localhost:8080/)
   - Recherchez `file_processing_dag` dans la liste des DAGs.

2. **D√©clencher le DAG manuellement** :
   - Cliquez sur le bouton "Trigger DAG" pour lancer l'ex√©cution.

3. **Surveiller l'ex√©cution** :
   - Acc√©dez √† l'onglet "Graph View" pour visualiser l'√©tat des t√¢ches.
   - V√©rifiez les logs de chaque t√¢che pour confirmer leur bon d√©roulement.

4. **V√©rifier les r√©sultats** :
   - Assurez-vous que le fichier CSV a √©t√© charg√© dans la base de donn√©es SQLite.
   - V√©rifiez que le fichier a √©t√© d√©plac√© dans le dossier `archive`.
   - Confirmez que les donn√©es ont √©t√© affich√©es correctement dans les logs.

---

## üîç V√©rification

Pour valider votre DAG :

1. **V√©rifiez la pr√©sence du DAG** dans l'interface web d'Airflow.
2. **D√©clenchez manuellement le DAG** via l'interface.
3. **Analysez les logs** pour chaque t√¢che pour confirmer leur bon d√©roulement.
4. **V√©rifiez les r√©sultats** :
   - Le fichier CSV a √©t√© charg√© dans la base de donn√©es.
   - Le fichier a √©t√© archiv√©.
   - Les donn√©es ont √©t√© affich√©es correctement.

---

## üí° Astuces

- Utilisez `catchup=False` pour √©viter l'ex√©cution des DAGs historiques.
- Testez votre DAG avec `airflow dags test [dag_id] [date]`.
- Assurez-vous que les chemins de fichiers sont corrects et accessibles par Airflow.

---

## Solution compl√®te

??? example "Afficher la solution"
    ```python
    from datetime import datetime, timedelta
    from airflow.models.dag import DAG
    from airflow.sensors.filesystem import FileSensor
    from airflow.operators.python import PythonOperator
    from airflow.operators.bash import BashOperator
    import pandas as pd
    import sqlite3
    import os

    default_args = {
        'owner': 'votre_nom',
        'retries': 2,
        'retry_delay': timedelta(minutes=5),
        'start_date': datetime(2023, 10, 1),
    }

    dag = DAG(
        'file_processing_dag',
        default_args=default_args,
        description='DAG pour surveiller et traiter un fichier CSV',
        schedule_interval=timedelta(days=1),
        catchup=False
    )

    task_file_sensor = FileSensor(
        task_id='file_sensor_task',
        filepath='data.csv',
        fs_conn_id='file_sensor_conn',
        poke_interval=30,
        timeout=300,
        mode='poke',
        dag=dag
    )

    def load_csv_to_db():
        csv_path = '/appdata/data.csv'
        db_path = '/appdata/stores.db'
        df = pd.read_csv(csv_path)
        conn = sqlite3.connect(db_path)
        df.to_sql('sales', conn, if_exists='append', index=False)
        conn.close()

    task_load_csv = PythonOperator(
        task_id='load_csv_task',
        python_callable=load_csv_to_db,
        dag=dag
    )

    task_archive_file = BashOperator(
        task_id='archive_file_task',
        bash_command='mv /appdata/data.csv /appdata/archive/data.csv',
        dag=dag
    )

    def display_data():
        db_path = '/appdata/stores.db'
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query('SELECT * FROM sales LIMIT 5', conn)
        print(df)
        conn.close()

    task_display_data = PythonOperator(
        task_id='display_data_task',
        python_callable=display_data,
        dag=dag
    )

    task_file_sensor >> task_load_csv >> task_archive_file >> task_display_data
    ```

---

## Conclusion

Vous avez maintenant un DAG fonctionnel qui surveille un dossier, traite un fichier CSV, et archive le fichier apr√®s traitement. Vous pouvez √©tendre ce DAG en ajoutant des fonctionnalit√©s suppl√©mentaires, comme la gestion des erreurs ou l'envoi de notifications.